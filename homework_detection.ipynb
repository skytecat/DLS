{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skytecat/DLS/blob/main/homework_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a7f379a-833c-4cfe-8860-3653c6fb0783",
      "metadata": {
        "id": "8a7f379a-833c-4cfe-8860-3653c6fb0783"
      },
      "source": [
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Домашнее задание. Детекция объектов</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f6062-7f0c-42b9-ae36-0f360c852d25",
      "metadata": {
        "id": "149f6062-7f0c-42b9-ae36-0f360c852d25"
      },
      "source": [
        "В этом домашнем задании мы продолжим работу над детектором из семинара, поэтому при необходимости можете заимствовать оттуда любой код.\n",
        "\n",
        "Домашнее задание можно разделить на следующие части:\n",
        "\n",
        "* Переделываем модель [4]\n",
        "  * Backbone[1],\n",
        "  * Neck [2],\n",
        "  * Head [1]\n",
        "* Label assignment [3]:\n",
        "  * TAL [3]\n",
        "* Лоссы [1]:\n",
        "  * CIoU loss [1]\n",
        "* Кто больше? [5]\n",
        "  * 0.05 mAP [1]\n",
        "  * 0.1 mAP  [2]\n",
        "  * 0.2 mAP [5]\n",
        "\n",
        "**Максимальный балл:** 10 баллов. (+3 балла бонус)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6fb023c6-434e-47b4-b048-73683a6ce482",
      "metadata": {
        "id": "6fb023c6-434e-47b4-b048-73683a6ce482"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "\n",
        "from PIL import Image\n",
        "import io\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e2b5b2-113e-43d5-a134-31b51a256671",
      "metadata": {
        "id": "42e2b5b2-113e-43d5-a134-31b51a256671"
      },
      "source": [
        "### Загрузка данных\n",
        "\n",
        "Мы продолжаем работу с датасетом из семинара - Halo infinite ([сслыка](https://universe.roboflow.com/graham-doerksen/halo-infinite-angel-aim)). Загрузка данных и создание датасета полностью скопированы из семинара.\n",
        "\n",
        "Сначала загружаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b2eecb64-16a2-4b97-b627-890ea316a594",
      "metadata": {
        "id": "b2eecb64-16a2-4b97-b627-890ea316a594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f3648e-d74b-4c16-de06-2e2d0b9a31b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "splits = {'train': 'data/train-00000-of-00001-0d6632d599c29801.parquet',\n",
        "          'validation': 'data/validation-00000-of-00001-c6b77a557eeedd52.parquet',\n",
        "          'test': 'data/test-00000-of-00001-866d29d8989ea915.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/Francesco/halo-infinite-angel-videogame/\" + splits[\"train\"])\n",
        "df_test = pd.read_parquet(\"hf://datasets/Francesco/halo-infinite-angel-videogame/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ade755e-4471-4c79-84bc-8b560484e833",
      "metadata": {
        "id": "1ade755e-4471-4c79-84bc-8b560484e833"
      },
      "source": [
        "Создаем датасет для предобработки данных"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "class HaloDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data  # pandas DataFrame\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # --- Изображение ---\n",
        "        image = Image.open(io.BytesIO(row[\"bytes\"]))\n",
        "        image = np.array(image)\n",
        "        # Переводим в [C, H, W] и нормализуем в [0, 1]\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # --- Боксы ---\n",
        "        boxes = row[\"boxes\"]  # ожидается: список или массив [[x1,y1,x2,y2], ...]\n",
        "        if isinstance(boxes, list):\n",
        "            boxes = np.array(boxes, dtype=np.float32)\n",
        "        boxes = torch.from_numpy(boxes).float()  # [N, 4]\n",
        "\n",
        "        # --- Метки ---\n",
        "        labels = row[\"labels\"]  # ожидается: список или массив [1, 3, ...]\n",
        "        if isinstance(labels, list):\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "        labels = torch.from_numpy(labels).long()  # [N]\n",
        "\n",
        "        # --- Проверка ---\n",
        "        assert boxes.ndim == 2 and boxes.shape[1] == 4, f\"boxes shape: {boxes.shape}\"\n",
        "        assert labels.ndim == 1, f\"labels shape: {labels.shape}\"\n",
        "        assert len(boxes) == len(labels), f\"boxes ({len(boxes)}) != labels ({len(labels)})\"\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "O8AV7XrLWJUO"
      },
      "id": "O8AV7XrLWJUO",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "VDFtO6IYXUgJ"
      },
      "id": "VDFtO6IYXUgJ",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5a2f8ba9-5ae2-4d33-9d9d-5a0f0bb36d38",
      "metadata": {
        "id": "5a2f8ba9-5ae2-4d33-9d9d-5a0f0bb36d38"
      },
      "outputs": [],
      "source": [
        "# class HaloDataset(Dataset):\n",
        "#     def __init__(self, dataframe, transform=None):\n",
        "#         df_objects = pd.json_normalize(dataframe['objects'])[[\"bbox\", \"category\"]]\n",
        "#         df_images = pd.json_normalize(dataframe['image'])[[\"bytes\"]]\n",
        "#         self.data = dataframe[[\"image_id\"]].join(df_objects).join(df_images)\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.data.iloc[idx]\n",
        "\n",
        "#         # Изображение\n",
        "#         image = Image.open(io.BytesIO(row[\"bytes\"]))\n",
        "#         image = np.array(image)\n",
        "#         image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0  # [C, H, W], float\n",
        "\n",
        "#         # Боксы (должны быть в формате XYXY, float32)\n",
        "#         boxes = torch.from_numpy(row[\"boxes\"]).float()  # shape: [N, 4]\n",
        "\n",
        "#         # Метки классов (должны быть long, и классы с 1, не с 0!)\n",
        "#         labels = torch.from_numpy(row[\"labels\"]).long()  # shape: [N]\n",
        "\n",
        "#         target = {\n",
        "#             'boxes': boxes,\n",
        "#             'labels': labels\n",
        "#         }\n",
        "\n",
        "#         return image, target\n",
        "\n",
        "#     # def __getitem__(self, idx):\n",
        "#     #     \"\"\"Загружаем данные и разметку для объекта с индексом `idx`.\n",
        "\n",
        "#     #     labels: List[int] Набор классов для каждого ббокса,\n",
        "#     #     boxes: List[List[int]] Набор ббоксов в формате (x_min, y_min, w, h).\n",
        "#     #     \"\"\"\n",
        "#     #     row = self.data.iloc[idx]\n",
        "#     #     image = Image.open(io.BytesIO(row[\"bytes\"]))\n",
        "#     #     image = np.array(image)\n",
        "\n",
        "#     #     target = {}\n",
        "#     #     target[\"image_id\"] = row[\"image_id\"]\n",
        "\n",
        "#     #     labels = [row[\"category\"]] if isinstance(row[\"category\"], int) else row['category']\n",
        "#     #     # Вычитаем единицу чтобы классы начинались с нуля\n",
        "#     #     labels = [label - 1 for label in labels]\n",
        "#     #     boxes = row['bbox'].tolist()\n",
        "\n",
        "#     #     if self.transform is not None:\n",
        "#     #         transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "#     #         image, boxes, labels = transformed[\"image\"], transformed[\"bboxes\"], transformed[\"labels\"]\n",
        "#     #     else:\n",
        "#     #         image = transforms.ToTensor()(image)\n",
        "\n",
        "#     #     target['boxes'] = torch.tensor(np.array(boxes), dtype=torch.float32)\n",
        "#     #     target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
        "#     #     return image, target\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     batch = tuple(zip(*batch))\n",
        "#     images = torch.stack(batch[0])\n",
        "#     return images, batch[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e864d642-45eb-4733-a763-2b2b7c711929",
      "metadata": {
        "id": "e864d642-45eb-4733-a763-2b2b7c711929"
      },
      "source": [
        "Чтобы модель не переобучалась, можно добавить больше аугментаций, весь список можно посмотреть тут [[ссылка](https://explore.albumentations.ai/)].\n",
        "\n",
        "Какие можно использовать аугментации?\n",
        "* Добавить зум `RandomResizedCrop`,\n",
        "* Сделать цветовые аугментации типа `RandomBrightnessContrast` и/или `HueSaturationValue`,\n",
        "* Добавить шум `GaussNoise`,\n",
        "* Вырезать случайные части изображения `CoarseDropout`,\n",
        "* И любые другие!\n",
        "\n",
        "Аугментации можно комбинировать посредствам `A.OneOf`, `A.SomeOf` или `A.RandomOrder`.\n",
        "\n",
        "Хоть аугментации ограничиваются только вашей фантазией, перед обучением советуем посмотреть на результат преобразований и убедиться, что изображение ещё поддается детекции:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c8c7095-9b77-4716-86cd-9b3e7dc3890c",
      "metadata": {
        "id": "9c8c7095-9b77-4716-86cd-9b3e7dc3890c"
      },
      "outputs": [],
      "source": [
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=30, p=0.5),\n",
        "        A.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    # Раскомментируй, если аугментации изменяют ббоксы.\n",
        "    # Не забудь указать верный формат для ббоксов.\n",
        "    bbox_params=A.BboxParams(format='coco', label_fields=['labels'])\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c6edb2-f020-413c-8bf5-6b149e0086b6",
      "metadata": {
        "id": "02c6edb2-f020-413c-8bf5-6b149e0086b6"
      },
      "source": [
        "Не забываем инициализировать наш датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e4a10a88-6df0-4d1a-8640-eaca2f12e511",
      "metadata": {
        "id": "e4a10a88-6df0-4d1a-8640-eaca2f12e511"
      },
      "outputs": [],
      "source": [
        "train_dataset = HaloDataset(df_train, transform=train_transform)\n",
        "test_dataset = HaloDataset(df_test, transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb463da-ab51-460c-bfb9-e95dddfff90f",
      "metadata": {
        "id": "adb463da-ab51-460c-bfb9-e95dddfff90f"
      },
      "source": [
        "## Переделываем модель [4 балла]\n",
        "\n",
        "В семинаре мы реализовали самый базовый детектор, а сейчас настало время его улучшать.\n",
        "\n",
        "### Backbone [1 балл]\n",
        "\n",
        "Хорошей практикой считается размораживать несколько последних слоев в backbone, это позволяет немного улучить качество модели. Давайте улушчим класс Backbone из лекции, добавив ему возможность разморозки __k__ последних слоев или блоков (на ваш выбор)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d92b9c64-73f8-4195-aa56-b9108589a312",
      "metadata": {
        "id": "d92b9c64-73f8-4195-aa56-b9108589a312"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "    def __init__(self, model_name=\"efficientnet_b0\", out_indices=(-1, -2, -3), unfreeze_last_k=0):\n",
        "        super().__init__()\n",
        "        # timm.list_models(pretrained=True)\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, features_only=True, out_indices=out_indices)\n",
        "        # Заморозим все\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Разморозим k последних блоков\n",
        "        if unfreeze_last_k > 0:\n",
        "          blocks = self.backbone.blocks\n",
        "          for block in blocks[-unfreeze_last_k:]:\n",
        "            for param in block.parameters():\n",
        "              param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3a3748-aa11-4bf8-9b38-59ff08366573",
      "metadata": {
        "id": "3b3a3748-aa11-4bf8-9b38-59ff08366573"
      },
      "source": [
        "### NECK [2 балла]\n",
        "\n",
        "Следующее улучшение коснется шеи. Предлагаем реализовать знакомую из лекции архитектуру FPN.\n",
        "\n",
        "#### Feature Pyramid Network\n",
        "\n",
        "<center><img src=\"https://user-images.githubusercontent.com/57972646/69858594-b14a6c00-12d5-11ea-8c3e-3c17063110d3.png\"/></center>\n",
        "\n",
        "\n",
        "* [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)\n",
        "\n",
        "Она состоит из top-down пути, в котором происходит 2 вещи:\n",
        "1. Увеличивается пространственная размерность фичей,\n",
        "2. С помощью скипконнекшеннов, добавляются фичи из backbone модели.\n",
        "\n",
        "Для увеличения пространственной размерности используется __nearest neighbor upsampling__, а фичи из шеи и бекбоуна суммируются.\n",
        "\n",
        "__TIPS__:\n",
        "* Можете использовать базовые классы из лекции,\n",
        "* Воспользуйтесь AnchorGenerator-ом, чтобы создавать якоря сразу для нескольких выходов,\n",
        "* Не забудьте использовать nn.ModuleList, если захотите сделать динамическое количество голов у модели,\n",
        "* Также, можно добавить доп конволюцию (3х3 с паддингом) у каждого выхода шеи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2119ccb8-7d21-4805-af5c-5a1d8d3a0e06",
      "metadata": {
        "id": "2119ccb8-7d21-4805-af5c-5a1d8d3a0e06"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Neck(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels=256):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            in_channels_list (List[int]): число каналов на выходе backbone для каждого уровня\n",
        "                                          Ожидается порядок от ГЛУБОКИХ к МЕЛКИМ признакам:\n",
        "                                          например, [2048, 1024, 512] для ResNet50 с out_indices=(-1, -2, -3)\n",
        "            out_channels (int): число каналов на выходе каждого уровня FPN\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        # Мы будем обрабатывать уровни от глубоких к мелким (C5 → C4 → C3)\n",
        "        # lateral_convs: 1x1 свёртки для выравнивания каналов\n",
        "        self.lateral_convs = nn.ModuleList()\n",
        "        # output_convs: 3x3 свёртки для сглаживания\n",
        "        self.output_convs = nn.ModuleList()\n",
        "\n",
        "        for in_channels in in_channels_list:\n",
        "            self.lateral_convs.append(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            )\n",
        "            self.output_convs.append(\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "            )\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features (List[Tensor]): признаки от backbone'а.\n",
        "                                     Порядок: [C5, C4, C3] — от глубоких к мелким\n",
        "        Returns:\n",
        "            fpn_features (List[Tensor]): [P3, P4, P5] — от мелких к глубоким\n",
        "        \"\"\"\n",
        "        # 1. Применяем lateral 1x1 свёртки\n",
        "        lateral_features = []\n",
        "        for i, feat in enumerate(features):\n",
        "            lateral_features.append(self.lateral_convs[i](feat))\n",
        "\n",
        "        # 2. Top-down путь: начинаем с самого глубокого уровня\n",
        "        # P5 = lateral(C5)\n",
        "        fpn_features = [lateral_features[0]]\n",
        "\n",
        "        # Идём от P5 к P4, P3, ...\n",
        "        for i in range(1, len(lateral_features)):\n",
        "            # Апсемплинг предыдущего уровня до размера текущего\n",
        "            upsampled = F.interpolate(\n",
        "                fpn_features[-1],\n",
        "                size=lateral_features[i].shape[-2:],  # (H, W)\n",
        "                mode='nearest'\n",
        "            )\n",
        "            # Суммируем с lateral feature из backbone'а\n",
        "            fpn = upsampled + lateral_features[i]\n",
        "            fpn_features.append(fpn)\n",
        "\n",
        "        # 3. Применяем post-processing 3x3 свёртки\n",
        "        output_features = []\n",
        "        for i, fpn in enumerate(fpn_features):\n",
        "            out = self.output_convs[i](fpn)\n",
        "            output_features.append(out)\n",
        "\n",
        "        # 4. Возвращаем в порядке от МЕЛКИХ к ГЛУБОКИМ: [P3, P4, P5]\n",
        "        return output_features[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531f59fe-8248-4b19-aa31-8acecccb0828",
      "metadata": {
        "id": "531f59fe-8248-4b19-aa31-8acecccb0828"
      },
      "source": [
        "### Head [1 балл]\n",
        "\n",
        "В качестве шеи можно выбрать __один из двух__ вариантов:\n",
        "\n",
        "#### 1. Decoupled Head\n",
        "\n",
        "Реализовать Decoupled Head из [YOLOX](https://arxiv.org/abs/2107.08430).\n",
        "<center><img src=\"https://i.ibb.co/BVtBR2R3/Decoupled-head.jpg\"/></center>\n",
        "\n",
        "**TIP**: Возьмите за основу голову из семинара, тк она сильно похожа на Decoupled Head.\n",
        "\n",
        "Изменять количество параметров у шей на разных уровнях не обязательно.\n",
        "\n",
        "#### 2. Confidence score free head\n",
        "\n",
        "Нужно взять за основу голову из семинара и полностью убрать предсказание confidence score. Чтобы модель предсказывала только 2 группы: ббоксы и классы.\n",
        "\n",
        "Есть следующие способы удаления confidence score:\n",
        "* Добавление нового класса ФОН. Обычно его обозначают нулевым классом.\n",
        "* Присваивание ббоксам БЕЗ объекта вектор из нулей в качестве таргета.\n",
        "\n",
        "Выберете тот, который вам больше нравится и будте внимательны при расчете лосса!\n",
        "\n",
        "**Важно!** Удаление confidence score повлияет на следующие методы из семинара:\n",
        "* target_assign\n",
        "* ComputeLoss\n",
        "* _filter_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f223b6ca-9498-4e75-9b97-6199c82977e1",
      "metadata": {
        "id": "f223b6ca-9498-4e75-9b97-6199c82977e1"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, in_channels, num_anchors, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_anchors = num_anchors\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.stem = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Отдельные ветки\n",
        "        # Ветка классификации\n",
        "        self.cls_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.cls_bn = nn.BatchNorm2d(in_channels)\n",
        "        self.cls_head = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=1)\n",
        "\n",
        "        # Ветка регрессии + confidence\n",
        "        self.reg_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "        self.reg_bn = nn.BatchNorm2d(in_channels)\n",
        "        self.reg_head = nn.Conv2d(in_channels, num_anchors * 5, kernel_size=1)  # 4 coord + 1 obj\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.stem(x))\n",
        "\n",
        "        # Classification branch\n",
        "        cls_feat = F.relu(self.cls_bn(self.cls_conv(x)))\n",
        "        cls_logits = self.cls_head(cls_feat)\n",
        "\n",
        "        # Regression + Objectness branch\n",
        "        reg_feat = F.relu(self.reg_bn(self.reg_conv(x)))\n",
        "        bbox_and_obj = self.reg_head(reg_feat)  # [B, A*5, H, W]\n",
        "\n",
        "        return cls_logits, bbox_and_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88025100-78a7-4aba-b4f5-e1547b170f24",
      "metadata": {
        "id": "88025100-78a7-4aba-b4f5-e1547b170f24"
      },
      "source": [
        "Теперь можно снова реализовать класс детектора с учетом всех частей выше!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4dd8bcb8-149f-4916-b37f-8ba0be54a4c9",
      "metadata": {
        "id": "4dd8bcb8-149f-4916-b37f-8ba0be54a4c9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "\n",
        "class Detector(nn.Module):\n",
        "    \"\"\"\n",
        "    Детектор с FPN и Decoupled Head.\n",
        "\n",
        "    Параметры\n",
        "    ---------\n",
        "    backbone_model_name : str\n",
        "        Имя timm модели для backbone.\n",
        "    neck_n_channels : int\n",
        "        Число каналов на выходе шеи (FPN).\n",
        "    num_classes : int\n",
        "        Число объектных классов (без фона!).\n",
        "    anchor_sizes : tuple of tuples\n",
        "        Размеры якорей для КАЖДОГО уровня FPN, например: ((32,), (64,), (128,))\n",
        "    anchor_ratios : tuple of tuples\n",
        "        Пропорции якорей для КАЖДОГО уровня, например: ((0.5, 1.0, 2.0),) * 3\n",
        "    input_size : tuple\n",
        "        Размер входного изображения (H, W).\n",
        "    unfreeze_last_k : int\n",
        "        Сколько последних блоков backbone'а разморозить.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 backbone_model_name=\"efficientnet_b0\",\n",
        "                 neck_n_channels=256,\n",
        "                 num_classes=4,\n",
        "                 anchor_sizes=((32,), (64,), (128,)),\n",
        "                 anchor_ratios=((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0)),\n",
        "                 input_size=(640, 640),\n",
        "                 unfreeze_last_k=0\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # === 1. BACKBONE (с несколькими выходами) ===\n",
        "        # В FPN обычно берут 3-4 уровня: out_indices=(-1, -2, -3)\n",
        "        self.backbone = Backbone(\n",
        "            model_name=backbone_model_name,\n",
        "            out_indices=(-1, -2, -3),\n",
        "            unfreeze_last_k=unfreeze_last_k\n",
        "        )\n",
        "\n",
        "        # Получаем число каналов каждого уровня\n",
        "        dummy = torch.randn(1, 3, input_size[0], input_size[1])\n",
        "        with torch.no_grad():\n",
        "            feats = self.backbone(dummy)\n",
        "            in_channels_list = [f.shape[1] for f in feats]  # [C5, C4, C3]\n",
        "\n",
        "        # === 2. NECK (FPN) ===\n",
        "        self.neck = Neck(in_channels_list=in_channels_list, out_channels=neck_n_channels)\n",
        "\n",
        "        # === 3. HEADS (по одному на уровень FPN) ===\n",
        "        num_levels = len(in_channels_list)\n",
        "        self.heads = nn.ModuleList([\n",
        "            Head(\n",
        "                in_channels=neck_n_channels,\n",
        "                num_anchors=len(anchor_sizes[i]) * len(anchor_ratios[i]),\n",
        "                num_classes=num_classes\n",
        "            )\n",
        "            for i in range(num_levels)\n",
        "        ])\n",
        "\n",
        "        # === 4. ANCHOR GENERATOR ===\n",
        "        self.anchor_generator = AnchorGenerator(\n",
        "            sizes=anchor_sizes,\n",
        "            aspect_ratios=anchor_ratios\n",
        "        )\n",
        "\n",
        "        # Предвычисляем якоря для фиксированного input_size\n",
        "        self._precompute_anchors(input_size)\n",
        "\n",
        "    def _precompute_anchors(self, input_size):\n",
        "        \"\"\"Предвычисляет якоря для заданного размера изображения.\"\"\"\n",
        "        # Получаем размеры feature maps после backbone + neck\n",
        "        dummy = torch.randn(1, 3, input_size[0], input_size[1])\n",
        "        with torch.no_grad():\n",
        "            backbone_feats = self.backbone(dummy)\n",
        "            fpn_feats = self.neck(backbone_feats)  # [P3, P4, P5]\n",
        "            grid_sizes = [f.shape[-2:] for f in fpn_feats]  # [(H3, W3), (H4, W4), (H5, W5)]\n",
        "\n",
        "        # Вычисляем strides (на сколько уменьшено изображение)\n",
        "        strides = [\n",
        "            [input_size[0] // h, input_size[1] // w]\n",
        "            for h, w in grid_sizes\n",
        "        ]\n",
        "\n",
        "        # Генерируем якоря\n",
        "        anchors_list = self.anchor_generator.grid_anchors(grid_sizes, strides)\n",
        "        anchors = torch.cat(anchors_list, dim=0)  # [N_total_anchors, 4]\n",
        "\n",
        "        # Сохраняем в буфере (чтобы не пересчитывать каждый раз)\n",
        "        self.register_buffer(\"anchors\", anchors)\n",
        "        self.register_buffer(\"anchor_centers\", (anchors[:, :2] + anchors[:, 2:]) / 2)\n",
        "        self.register_buffer(\"anchor_sizes\", anchors[:, 2:] - anchors[:, :2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: тензор [B, C, H, W]\n",
        "        Returns:\n",
        "            В зависимости от режима:\n",
        "            - training: списки предсказаний по уровням,\n",
        "            - eval: декодированные боксы и вероятности.\n",
        "        \"\"\"\n",
        "        # 1. Backbone\n",
        "        backbone_feats = self.backbone(x)  # [C5, C4, C3]\n",
        "\n",
        "        # 2. Neck (FPN)\n",
        "        fpn_feats = self.neck(backbone_feats)  # [P3, P4, P5]\n",
        "\n",
        "        # 3. Heads\n",
        "        all_cls_logits = []\n",
        "        all_bbox_preds = []\n",
        "        all_confidence_logits = []\n",
        "\n",
        "        for i, feat in enumerate(fpn_feats):\n",
        "            cls_logits, bbox_and_obj = self.heads[i](feat)\n",
        "            # Разделяем bbox и confidence\n",
        "            bbox_offsets = bbox_and_obj[:, :4 * self.heads[i].num_anchors, :, :]\n",
        "            confidence_logits = bbox_and_obj[:, 4 * self.heads[i].num_anchors:, :, :]\n",
        "\n",
        "            all_cls_logits.append(cls_logits)\n",
        "            all_bbox_preds.append(bbox_offsets)\n",
        "            all_confidence_logits.append(confidence_logits)\n",
        "\n",
        "        if self.training:\n",
        "            # Возвращаем \"сырые\" предсказания по уровням\n",
        "            return {\n",
        "                'cls_logits': all_cls_logits,\n",
        "                'bbox_offsets': all_bbox_preds,\n",
        "                'confidence_logits': all_confidence_logits\n",
        "            }\n",
        "\n",
        "        # === Inference: декодируем все боксы ===\n",
        "        all_bboxes = []\n",
        "        all_confidences = []\n",
        "        all_cls_probs = []\n",
        "\n",
        "        start_idx = 0\n",
        "        for i, feat in enumerate(fpn_feats):\n",
        "            B, _, H, W = feat.shape\n",
        "            num_anchors_level = self.heads[i].num_anchors\n",
        "            num_preds = H * W * num_anchors_level\n",
        "\n",
        "            # Берём смещения для этого уровня\n",
        "            bbox_offsets_flat = all_bbox_preds[i].permute(0, 2, 3, 1).contiguous().view(B, -1, 4)\n",
        "            conf_flat = all_confidence_logits[i].permute(0, 2, 3, 1).contiguous().view(B, -1)\n",
        "            cls_flat = all_cls_logits[i].permute(0, 2, 3, 1).contiguous().view(B, -1, self.num_classes)\n",
        "\n",
        "            # Декодируем боксы\n",
        "            anchors_level = self.anchors[start_idx:start_idx + num_preds]\n",
        "            bboxes = self._decode_level(bbox_offsets_flat, anchors_level)\n",
        "\n",
        "            all_bboxes.append(bboxes)\n",
        "            all_confidences.append(torch.sigmoid(conf_flat))\n",
        "            all_cls_probs.append(torch.softmax(cls_flat, dim=-1))\n",
        "\n",
        "            start_idx += num_preds\n",
        "\n",
        "        # Объединяем все уровни\n",
        "        final_bboxes = torch.cat(all_bboxes, dim=1)\n",
        "        final_confidences = torch.cat(all_confidences, dim=1)\n",
        "        final_cls_probs = torch.cat(all_cls_probs, dim=1)\n",
        "\n",
        "        return final_bboxes, final_confidences, final_cls_probs\n",
        "\n",
        "    def _decode_level(self, bbox_offsets, anchors):\n",
        "        \"\"\"Декодирует боксы для одного уровня.\"\"\"\n",
        "        tx = bbox_offsets[:, :, 0]\n",
        "        ty = bbox_offsets[:, :, 1]\n",
        "        tw = bbox_offsets[:, :, 2]\n",
        "        th = bbox_offsets[:, :, 3]\n",
        "\n",
        "        anchor_centers = (anchors[:, :2] + anchors[:, 2:]) / 2\n",
        "        anchor_wh = anchors[:, 2:] - anchors[:, :2]\n",
        "\n",
        "        cx = anchor_centers[:, 0] + torch.sigmoid(tx) * anchor_wh[:, 0]\n",
        "        cy = anchor_centers[:, 1] + torch.sigmoid(ty) * anchor_wh[:, 1]\n",
        "        w = torch.exp(tw) * anchor_wh[:, 0]\n",
        "        h = torch.exp(th) * anchor_wh[:, 1]\n",
        "\n",
        "        x_min = cx - w / 2\n",
        "        y_min = cy - h / 2\n",
        "        return torch.stack([x_min, y_min, w, h], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc104b6-0960-404d-805c-ec880dc9bfb0",
      "metadata": {
        "id": "9bc104b6-0960-404d-805c-ec880dc9bfb0"
      },
      "source": [
        "## Label assignment [3 балла]\n",
        "В этой секции предлагается заменить функцию `assign_target` на более современный алгоритм который называется Task alignment learning.\n",
        "\n",
        "Он описан в статье [TOOD](https://arxiv.org/abs/2108.07755) в секции 3.2. Для удобства вот его основные шаги:\n",
        "\n",
        "1. Посчитать значение метрики для каждого предсказанного ббокса:\n",
        "    \n",
        "$$t = s^\\alpha * u^\\beta$$\n",
        "    \n",
        "где,\n",
        "* $s$ — classification score, или вероятность принадлежности предсказанного ббокса к классу реального ббокса (**GT**);\n",
        "* $u$ — IoU между предсказанным и реальным ббоксами;\n",
        "* $\\alpha,\\ \\beta$ — нормализационные константы, обычно $\\alpha = 6.0, \\ \\beta = 1.0$.\n",
        "    \n",
        "2. Отфильтровать предсказания на основе **GT**.\n",
        "\n",
        "    Для якорных детекторов, обычно, выбираются только те предсказания, центры якорей которых находятся внутри GT.\n",
        "4. Для каждого **GT** выбрать несколько (обычно 5 или 13) самых подходящих предсказаний.\n",
        "5. Если предсказание рассматривается в качестве подходящего для нескольких **GT** — выбрать **GT** с наибольшим пересечением по IoU.\n",
        "\n",
        "\n",
        "**BAЖНО**: если будете использовать Runner из лекции, не забудьте поменять параметры  в `self.assign_target_method` в методе `_run_train_epoch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a88e418e-2fd5-4f7b-9675-f91db3c52ec5",
      "metadata": {
        "id": "a88e418e-2fd5-4f7b-9675-f91db3c52ec5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "    \"\"\"Вычисляет IoU между двумя наборами боксов.\n",
        "\n",
        "    Args:\n",
        "        box1: [N, 4] в формате (x1, y1, x2, y2)\n",
        "        box2: [M, 4] в формате (x1, y1, x2, y2)\n",
        "    Returns:\n",
        "        iou: [N, M]\n",
        "    \"\"\"\n",
        "    # Площади\n",
        "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
        "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
        "\n",
        "    # Пересечение\n",
        "    lt = torch.max(box1[:, None, :2], box2[:, :2])  # [N, M, 2]\n",
        "    rb = torch.min(box1[:, None, 2:], box2[:, 2:])  # [N, M, 2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N, M, 2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N, M]\n",
        "\n",
        "    iou = inter / (area1[:, None] + area2 - inter + eps)\n",
        "    return iou\n",
        "\n",
        "def TAL_assigner(\n",
        "    anchors,          # [num_anchors, 4] — все якоря в (x1, y1, x2, y2)\n",
        "    gt_boxes,         # [num_gts, 4] — GT боксы в (x1, y1, x2, y2)\n",
        "    gt_labels,        # [num_gts] — метки классов (0 не используется, классы с 1)\n",
        "    cls_probs,        # [num_anchors, num_classes] — вероятности классов (после sigmoid/softmax)\n",
        "    pred_boxes,       # [num_anchors, 4] — предсказанные боксы (x1, y1, x2, y2)\n",
        "    topk=13,          # сколько якорей выбрать на GT\n",
        "    alpha=6.0,\n",
        "    beta=1.0,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Task Alignment Learning (TAL) assigner из TOOD.\n",
        "\n",
        "    Returns:\n",
        "        pos_mask: [num_anchors] — булев маска положительных якорей\n",
        "        assigned_gt_inds: [num_anchors] — индекс GT для каждого якоря (-1 = фон)\n",
        "        assigned_labels: [num_anchors] — метка класса для каждого якоря (0 = фон)\n",
        "    \"\"\"\n",
        "    num_anchors = anchors.size(0)\n",
        "    num_gts = gt_boxes.size(0)\n",
        "\n",
        "    if num_gts == 0:\n",
        "        return (\n",
        "            torch.zeros(num_anchors, dtype=torch.bool, device=device),\n",
        "            torch.full((num_anchors,), -1, dtype=torch.long, device=device),\n",
        "            torch.zeros(num_anchors, dtype=torch.long, device=device)\n",
        "        )\n",
        "\n",
        "    # 1. Фильтрация: якоря, центры которых внутри GT\n",
        "    anchor_centers = (anchors[:, :2] + anchors[:, 2:]) / 2  # [num_anchors, 2]\n",
        "    # Проверяем, что центр якоря внутри хотя бы одного GT\n",
        "    is_anchor_in_gt = (\n",
        "        (anchor_centers[:, 0] >= gt_boxes[:, 0].min()) &\n",
        "        (anchor_centers[:, 0] <= gt_boxes[:, 2].max()) &\n",
        "        (anchor_centers[:, 1] >= gt_boxes[:, 1].min()) &\n",
        "        (anchor_centers[:, 1] <= gt_boxes[:, 3].max())\n",
        "    )  # Это упрощение! Лучше по каждому GT.\n",
        "\n",
        "    # Более точная проверка: центр якоря внутри КАЖДОГО GT\n",
        "    is_anchor_in_gt = (\n",
        "        (anchor_centers[:, None, 0] >= gt_boxes[:, 0]) &  # x >= x1\n",
        "        (anchor_centers[:, None, 0] <= gt_boxes[:, 2]) &  # x <= x2\n",
        "        (anchor_centers[:, None, 1] >= gt_boxes[:, 1]) &  # y >= y1\n",
        "        (anchor_centers[:, None, 1] <= gt_boxes[:, 3])    # y <= y2\n",
        "    )  # [num_anchors, num_gts]\n",
        "\n",
        "    # 2. Вычисляем IoU между предсказанными боксами и GT\n",
        "    ious = bbox_iou(pred_boxes, gt_boxes)  # [num_anchors, num_gts]\n",
        "\n",
        "    # 3. Получаем classification score для каждого (якорь, GT)\n",
        "    # cls_probs: [num_anchors, num_classes]\n",
        "    # gt_labels: [num_gts] — метки от 1 до num_classes\n",
        "    cls_scores = cls_probs[torch.arange(num_anchors)[:, None], gt_labels]  # [num_anchors, num_gts]\n",
        "\n",
        "    # 4. Вычисляем TAL score: t = s^α * u^β\n",
        "    alignment_metrics = (cls_scores ** alpha) * (ious ** beta)  # [num_anchors, num_gts]\n",
        "\n",
        "    # 5. Применяем маску: только якоря, центры которых внутри GT\n",
        "    alignment_metrics = alignment_metrics * is_anchor_in_gt.float()  # [num_anchors, num_gts]\n",
        "\n",
        "    # 6. Для каждого GT выбираем topk якорей по alignment_metrics\n",
        "    # topk_anchors: [num_gts, topk]\n",
        "    topk_metrics, topk_indices = torch.topk(\n",
        "        alignment_metrics.T,  # [num_gts, num_anchors]\n",
        "        k=min(topk, num_anchors),\n",
        "        dim=1,\n",
        "        largest=True\n",
        "    )\n",
        "\n",
        "    # 7. Назначаем GT каждому якорю\n",
        "    assigned_gt_inds = torch.full((num_anchors,), -1, dtype=torch.long, device=device)\n",
        "    assigned_labels = torch.zeros(num_anchors, dtype=torch.long, device=device)\n",
        "\n",
        "    # Для каждого GT\n",
        "    for gt_idx in range(num_gts):\n",
        "        anchor_indices = topk_indices[gt_idx]  # [topk]\n",
        "        # Назначаем GT этим якорям\n",
        "        for anchor_idx in anchor_indices:\n",
        "            # Если якорь уже назначен другому GT — выбираем по IoU\n",
        "            if assigned_gt_inds[anchor_idx] != -1:\n",
        "                # Сравниваем IoU с текущим и предыдущим GT\n",
        "                current_iou = ious[anchor_idx, gt_idx]\n",
        "                prev_gt = assigned_gt_inds[anchor_idx]\n",
        "                prev_iou = ious[anchor_idx, prev_gt]\n",
        "                if current_iou > prev_iou:\n",
        "                    assigned_gt_inds[anchor_idx] = gt_idx\n",
        "                    assigned_labels[anchor_idx] = gt_labels[gt_idx]\n",
        "            else:\n",
        "                assigned_gt_inds[anchor_idx] = gt_idx\n",
        "                assigned_labels[anchor_idx] = gt_labels[gt_idx]\n",
        "\n",
        "    pos_mask = assigned_gt_inds != -1\n",
        "\n",
        "    return pos_mask, assigned_gt_inds, assigned_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f25b75-58ed-44e7-adca-c5d518a7467b",
      "metadata": {
        "id": "28f25b75-58ed-44e7-adca-c5d518a7467b"
      },
      "source": [
        "### DIoU [1]\n",
        "\n",
        "Вместо SmoothL1, который используется в семинаре, реализуем лосс, основанный на пересечении ббоксов. В качестве тренировки давайте напишем Distance Intersection over Union (DIoU).\n",
        "\n",
        "<center><img src=https://wikidocs.net/images/page/163613/Free_Fig_5.png></center>\n",
        "\n",
        "Для его реализации разобъем задачу на части:\n",
        "\n",
        "**1. Реализуем IoU:**\n",
        "\n",
        "Пусть даны координаты для предсказанного ($B^p$) и истинного ($B^g$) ббоксов в формате XYXY или VOC PASCAL (левый верхний и правый нижний углы):\n",
        "\n",
        "$B^p=(x^p_1, y^p_1, x^p_2, y^p_2)$, $B^g=(x^g_1, y^g_1, x^g_2, y^g_2)$, тогда алгоритм расчета будет следующий:\n",
        "\n",
        "    1. Найдем площади обоих ббоксов:\n",
        "$$ A^p = (x^p_2 - x^p_1) * (y^p_2 - y^p_1) $$\n",
        "$$ A^g = (x^g_2 - x^g_1) * (y^g_2 - y^g_1) $$\n",
        "\n",
        "    2. Посчитаем пересечение между ббоксами:\n",
        "\n",
        "Тут мы предлагаем вам подумать как в общем виде можно расчитать размеры ббокса, который будет являться пересечением $B^p$ и $B^g$, а затем посчитать его площадь:\n",
        "\n",
        "$$x^I_1 = \\qquad \\qquad y^I_1 = $$\n",
        "$$x^I_2 = \\qquad \\qquad y^I_2 = $$\n",
        "\n",
        "В общем виде, площать будет записываться следующим образом:\n",
        "\n",
        "Если $x^I_2 > x^I_1$ & $y^I_2 > y^I_1$, тогда:\n",
        "\n",
        "$$I = (x^I_2 - x^I_1) * (y^I_2 - y^I_1)$$\n",
        "\n",
        "Иначе, $I = 0$.\n",
        "\n",
        "    3. Считаем объединение ббоксов.\n",
        "\n",
        "Мы можем посчитать эту площадь как сумму площадей двух ббоксов минус площадь пересечения (тк мы считаем её два раз в сумме площадей):\n",
        "\n",
        "$$U = A^p + A^g - I$$\n",
        "\n",
        "    4. Вычисляем IoU.\n",
        "\n",
        "$$IoU = \\frac{I}{U}$$\n",
        "\n",
        "**2. Посчитаем диагональ выпуклой оболочки:**\n",
        "\n",
        "Для расчета диагонали, сначала выпишите координаты верхнего левого и правого нижнего углов. Подумайте, чему будут равны эти координаты в общем случае?\n",
        "\n",
        "$$x^c_1 = \\qquad \\qquad y^c_1 = $$\n",
        "$$x^c_2 = \\qquad \\qquad y^c_2 = $$\n",
        "\n",
        "Подсказка: Нарисуйте несколько вариантов пересечений предсказания и GT на бумажке, и выпишите координаты для выпуклой оболочки.\n",
        "\n",
        "Тогда квадрат диагонали можно посчитать по формуле:\n",
        "\n",
        "$$c^2 = (x^c_2 - x^c_1)^2 + (y^c_2 - y^c_1)^2$$\n",
        "\n",
        "**3. Рассчитаем расстояние между цетрами ббоксов:**\n",
        "\n",
        "Сначала находим координаты центров каждого из ббоксов (если ббоксы в формате YOLO, то и считать ничего не нужно), затем считаем Евклидово расстояние между центрами.\n",
        "\n",
        "$d = $\n",
        "\n",
        "Собираем все части вместе и считаем лосс по формуле:\n",
        "\n",
        "$$ DIoU = 1 - IoU + \\frac{d^2}{c^2}$$\n",
        "\n",
        "Помните, что пар ббоксов может быть много! Возвращайте усредненное значение лосса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0c3740d0-76f7-43b6-a4bd-5ab7048d0c70",
      "metadata": {
        "id": "0c3740d0-76f7-43b6-a4bd-5ab7048d0c70"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import distance_box_iou_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1c412008-e90e-4087-9725-6f4777dc3c8b",
      "metadata": {
        "id": "1c412008-e90e-4087-9725-6f4777dc3c8b"
      },
      "outputs": [],
      "source": [
        "def gen_bbox(num_boxes=10):\n",
        "    min_corner = torch.randint(0, 100, (num_boxes, 2))\n",
        "    max_corner = torch.randint(50, 150, (num_boxes, 2))\n",
        "\n",
        "    for i in range(2):\n",
        "        wrong_order = min_corner[:, i] > max_corner[:, i]\n",
        "        if wrong_order.any():\n",
        "            min_corner[wrong_order, i], max_corner[wrong_order, i] = max_corner[wrong_order, i], min_corner[wrong_order, i]\n",
        "    return torch.cat((min_corner, max_corner), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6c764214-2797-4111-8e5f-d421dad9aae6",
      "metadata": {
        "id": "6c764214-2797-4111-8e5f-d421dad9aae6"
      },
      "outputs": [],
      "source": [
        "pred_boxes = gen_bbox(num_boxes=100)\n",
        "true_boxes = gen_bbox(num_boxes=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "24a64210-29a0-4b1a-a0d1-c690b1955801",
      "metadata": {
        "id": "24a64210-29a0-4b1a-a0d1-c690b1955801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d01bf7a-2cd3-4f0f-dc80-b6426b3380ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DIoU: 1.0210096836090088\n"
          ]
        }
      ],
      "source": [
        "print(f\" DIoU: {distance_box_iou_loss(pred_boxes, true_boxes, reduction=\"mean\").item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2dd590f7-59fc-4aae-93cd-23dc01d8cf3d",
      "metadata": {
        "id": "2dd590f7-59fc-4aae-93cd-23dc01d8cf3d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def diou_loss(pred_boxes, gt_boxes):\n",
        "    \"\"\"\n",
        "    DIoU Loss для боксов в формате XYXY (x1, y1, x2, y2).\n",
        "\n",
        "    Args:\n",
        "        pred_boxes: [N, 4] — предсказанные боксы\n",
        "        gt_boxes:   [N, 4] — истинные боксы\n",
        "\n",
        "    Returns:\n",
        "        mean DIoU loss (scalar)\n",
        "    \"\"\"\n",
        "    # 1. Площади предсказанных и истинных боксов\n",
        "    pred_w = pred_boxes[:, 2] - pred_boxes[:, 0]\n",
        "    pred_h = pred_boxes[:, 3] - pred_boxes[:, 1]\n",
        "    gt_w = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
        "    gt_h = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
        "\n",
        "    pred_area = pred_w * pred_h  # [N]\n",
        "    gt_area = gt_w * gt_h        # [N]\n",
        "\n",
        "    # 2. Площадь пересечения\n",
        "    # Координаты пересечения\n",
        "    xI1 = torch.max(pred_boxes[:, 0], gt_boxes[:, 0])\n",
        "    yI1 = torch.max(pred_boxes[:, 1], gt_boxes[:, 1])\n",
        "    xI2 = torch.min(pred_boxes[:, 2], gt_boxes[:, 2])\n",
        "    yI2 = torch.min(pred_boxes[:, 3], gt_boxes[:, 3])\n",
        "\n",
        "    # Размеры пересечения\n",
        "    inter_w = (xI2 - xI1).clamp(min=0)\n",
        "    inter_h = (yI2 - yI1).clamp(min=0)\n",
        "    inter_area = inter_w * inter_h  # [N]\n",
        "\n",
        "    # 3. Объединение\n",
        "    union_area = pred_area + gt_area - inter_area  # [N]\n",
        "\n",
        "    # 4. IoU\n",
        "    iou = inter_area / (union_area + 1e-7)  # [N]\n",
        "\n",
        "    # 5. Диагональ наименьшего охватывающего прямоугольника\n",
        "    xc1 = torch.min(pred_boxes[:, 0], gt_boxes[:, 0])\n",
        "    yc1 = torch.min(pred_boxes[:, 1], gt_boxes[:, 1])\n",
        "    xc2 = torch.max(pred_boxes[:, 2], gt_boxes[:, 2])\n",
        "    yc2 = torch.max(pred_boxes[:, 3], gt_boxes[:, 3])\n",
        "\n",
        "    c_sq = (xc2 - xc1) ** 2 + (yc2 - yc1) ** 2  # [N]\n",
        "\n",
        "    # 6. Расстояние между центрами\n",
        "    pred_center_x = (pred_boxes[:, 0] + pred_boxes[:, 2]) / 2\n",
        "    pred_center_y = (pred_boxes[:, 1] + pred_boxes[:, 3]) / 2\n",
        "    gt_center_x = (gt_boxes[:, 0] + gt_boxes[:, 2]) / 2\n",
        "    gt_center_y = (gt_boxes[:, 1] + gt_boxes[:, 3]) / 2\n",
        "\n",
        "    d_sq = (pred_center_x - gt_center_x) ** 2 + (pred_center_y - gt_center_y) ** 2  # [N]\n",
        "\n",
        "    # 7. DIoU\n",
        "    diou = 1 - iou + d_sq / (c_sq + 1e-7)  # [N]\n",
        "\n",
        "    return diou.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "169ebd6b-c45f-4b7d-a456-ff3e95619136",
      "metadata": {
        "id": "169ebd6b-c45f-4b7d-a456-ff3e95619136"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "pred_boxes = gen_bbox(num_boxes=1000)\n",
        "true_boxes = gen_bbox(num_boxes=1000)\n",
        "\n",
        "# проверим что написанный лосс выдает те же результаты что и лосс из торча.\n",
        "assert np.isclose(diou_loss(pred_boxes, true_boxes), distance_box_iou_loss(pred_boxes, true_boxes, reduction=\"mean\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1167d47f-9a56-40e2-9bb2-f8e749c5e499",
      "metadata": {
        "id": "1167d47f-9a56-40e2-9bb2-f8e749c5e499"
      },
      "source": [
        "## Кто больше? [5 баллов]\n",
        "\n",
        "Наконец то мы дошли до самый интересной части. Тут мы раздаем очки за mAP'ы!\n",
        "\n",
        "Все что вы написали выше вам поможет улучшить качество итогового детектора, настало время узнать насколько сильно :)\n",
        "\n",
        "За достижения порога по mAP на тестовом наборе вы получаете баллы:\n",
        "* 0.05 mAP [1]\n",
        "* 0.1 mAP [2]\n",
        "* 0.2 mAP [5]\n",
        "\n",
        "\n",
        "**TIPS**:\n",
        "1. На семинаре мы специально не унифицировали формат ббоксов между методами, чтобы обратить ваше внимание что за этим нужно следить. Чтобы было проще, сразу унифицируете формат по всему ноутбуку. Советуем использовать формат xyxy, тк IoU и NMS из torch используют именно этот формат. (Не забудьте поменять формат у таргета в `HaloDataset`).\n",
        "\n",
        "2. Попробуйте перейти к IoU-based лоссу при обучении. То есть обучать не смещения, а сразу предсказывать ббокс.\n",
        "\n",
        "3. Поэксперементируйте с подходами target assignment'а в процессе обучения. Например, можно на первых итерациях использовать обычный метод, а затем подключить TAL.\n",
        "\n",
        "4. Добавьте аугментаций!\n",
        "\n",
        "Можно взять [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/), библиотеку, которую мы использовали всеминаре. Или базовые аугментации из торча [тык](https://pytorch.org/vision/main/transforms.html). Если будете использовать торч, не забудте про ббоксы, transforms из коробки не будет их агументировать.\n",
        "\n",
        "5. Можете реализовать другую шею, которую мы обсуждали на лекции [Path Aggregation Network](https://arxiv.org/abs/1803.01534) она точно улучшит ваше итоговое качество.\n",
        "\n",
        "6. Попробуйте добавлять различные блоки из YOLO архитектур в шею вместо единичных конволюционных слоев. (Например, замените конволюции 3х3 на CSP блоки).\n",
        "\n",
        "7. Попробуйте заменить NMS на другой метод (WeightedNMS, SoftNMS, etc.). Немного ссылок:\n",
        "    * Статья про SoftNMS [тык](https://arxiv.org/pdf/1704.04503)\n",
        "    * Статья про WeightedNMS [тык](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w14/Zhou_CAD_Scale_Invariant_ICCV_2017_paper.pdf)\n",
        "    * Есть их реализация, правда на нумбе [git](https://github.com/ZFTurbo/Weighted-Boxes-Fusion?tab=readme-ov-file)\n",
        "\n",
        "8. Не бойтесь эксперементировать и удачи!\n",
        "\n",
        "Также, напишите развернутые ответы на следующие вопросы:\n",
        "\n",
        "**Questions:**\n",
        "1. Какой метод label assignment'a помогает лучше обучаться модели? Почему?\n",
        "2. Какое из сделаных вами улучшений внесло наибольший вклад в качество модели? Как вы думаете, почему это произошло?\n",
        "3. Какое из сделанных вами улучшений вообще не изменило метрику? Как вы думаете, почему это произошло?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4f0b3ca5-c7cb-4bfd-a999-a88df9ee56b8",
      "metadata": {
        "id": "4f0b3ca5-c7cb-4bfd-a999-a88df9ee56b8"
      },
      "outputs": [],
      "source": [
        "def filter_predictions(\n",
        "    outputs,\n",
        "    score_threshold=0.1,\n",
        "    nms_threshold=0.5,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Преобразует сырые выходы модели в формат [{'boxes', 'scores', 'labels'}, ...]\n",
        "    \"\"\"\n",
        "    all_predictions = []\n",
        "    bboxes, confidences, cls_probs = outputs  # [B, N, 4], [B, N], [B, N, C]\n",
        "\n",
        "    B = bboxes.shape[0]\n",
        "    for i in range(B):\n",
        "        # Фильтрация по confidence\n",
        "        valid_mask = confidences[i] > score_threshold\n",
        "        if valid_mask.sum() == 0:\n",
        "            all_predictions.append({\n",
        "                'boxes': torch.empty(0, 4, device=device),\n",
        "                'scores': torch.empty(0, device=device),\n",
        "                'labels': torch.empty(0, dtype=torch.long, device=device)\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        boxes_i = bboxes[i][valid_mask]\n",
        "        scores_i = confidences[i][valid_mask]\n",
        "        cls_probs_i = cls_probs[i][valid_mask]\n",
        "\n",
        "        # Получаем метки и вероятности классов\n",
        "        class_scores, class_labels = cls_probs_i.max(dim=1)\n",
        "        final_scores = scores_i * class_scores  # TOOD-style: obj * cls\n",
        "\n",
        "        # NMS\n",
        "        keep = torch.ops.torchvision.nms(\n",
        "            boxes_i, final_scores, nms_threshold\n",
        "        )\n",
        "\n",
        "        all_predictions.append({\n",
        "            'boxes': boxes_i[keep],\n",
        "            'scores': final_scores[keep],\n",
        "            'labels': class_labels[keep]\n",
        "        })\n",
        "\n",
        "    return all_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detection_loss(\n",
        "    model_outputs,\n",
        "    targets,\n",
        "    anchors,\n",
        "    alpha=6.0,\n",
        "    beta=1.0,\n",
        "    topk=13,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Общий лосс: cls_loss + diou_loss + obj_loss\n",
        "    \"\"\"\n",
        "    cls_logits_list = model_outputs['cls_logits']      # List[Tensor]\n",
        "    bbox_offsets_list = model_outputs['bbox_offsets']  # List[Tensor]\n",
        "    confidence_logits_list = model_outputs['confidence_logits']  # List[Tensor]\n",
        "\n",
        "    # Объединяем все уровни в один тензор\n",
        "    all_cls_logits = torch.cat([x.permute(0, 2, 3, 1).contiguous().view(-1, model_outputs['num_classes']) for x in cls_logits_list], dim=0)\n",
        "    all_bbox_offsets = torch.cat([x.permute(0, 2, 3, 1).contiguous().view(-1, 4) for x in bbox_offsets_list], dim=0)\n",
        "    all_conf_logits = torch.cat([x.permute(0, 2, 3, 1).contiguous().view(-1) for x in confidence_logits_list], dim=0)\n",
        "\n",
        "    # Предсказанные боксы (в формате XYXY)\n",
        "    pred_boxes = model.decode_bboxes(all_bbox_offsets)  # [N, 4]\n",
        "    cls_probs = torch.sigmoid(all_cls_logits)           # [N, C]\n",
        "    obj_probs = torch.sigmoid(all_conf_logits)          # [N]\n",
        "\n",
        "    # TAL assigner\n",
        "    gt_boxes = torch.cat([t['boxes'] for t in targets], dim=0)  # [M, 4]\n",
        "    gt_labels = torch.cat([t['labels'] for t in targets], dim=0)  # [M]\n",
        "\n",
        "    if gt_boxes.numel() == 0:\n",
        "        # Нет объектов — лосс только на фон\n",
        "        obj_loss = F.binary_cross_entropy_with_logits(all_conf_logits, torch.zeros_like(all_conf_logits))\n",
        "        cls_loss = F.binary_cross_entropy_with_logits(all_cls_logits, torch.zeros_like(all_cls_logits))\n",
        "        return obj_loss + cls_loss\n",
        "\n",
        "    pos_mask, assigned_gt_inds, assigned_labels = TAL_assigner(\n",
        "        anchors=anchors,\n",
        "        gt_boxes=gt_boxes,\n",
        "        gt_labels=gt_labels,\n",
        "        cls_probs=cls_probs,\n",
        "        pred_boxes=pred_boxes,\n",
        "        topk=topk,\n",
        "        alpha=alpha,\n",
        "        beta=beta,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # DIoU Loss только для положительных\n",
        "    if pos_mask.any():\n",
        "        diou_l = diou_loss(pred_boxes[pos_mask], gt_boxes[assigned_gt_inds[pos_mask]])\n",
        "    else:\n",
        "        diou_l = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # Objectness loss\n",
        "    obj_targets = pos_mask.float()\n",
        "    obj_loss = F.binary_cross_entropy_with_logits(all_conf_logits, obj_targets)\n",
        "\n",
        "    # Classification loss (focal loss можно добавить)\n",
        "    cls_targets = torch.zeros_like(cls_probs)\n",
        "    if pos_mask.any():\n",
        "        cls_targets[pos_mask, assigned_labels[pos_mask] - 1] = 1.0  # классы с 1, индексы с 0\n",
        "    cls_loss = F.binary_cross_entropy_with_logits(all_cls_logits, cls_targets)\n",
        "\n",
        "    return diou_l + obj_loss + cls_loss"
      ],
      "metadata": {
        "id": "vDIpWzFgQlid"
      },
      "id": "vDIpWzFgQlid",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, dataloader, device=\"cpu\", score_threshold=0.1, nms_threshold=0.5):\n",
        "    model.eval()\n",
        "    metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\")\n",
        "\n",
        "    for images, targets in tqdm(dataloader, desc=\"Validation\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)  # в eval режиме\n",
        "        predicts = filter_predictions(\n",
        "            outputs, score_threshold, nms_threshold, device=device\n",
        "        )\n",
        "        # targets уже в формате [{'boxes', 'labels'}, ...]\n",
        "        metric.update(predicts, targets)\n",
        "\n",
        "    return metric.compute()[\"map\"].item()"
      ],
      "metadata": {
        "id": "rNs39dYEQtLO"
      },
      "id": "rNs39dYEQtLO",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp6LeJpmRCyE",
        "outputId": "a4793380-c2ae-4281-8744-aaafff27b140"
      },
      "id": "Zp6LeJpmRCyE",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "\n",
        "def train_detector(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    anchors,\n",
        "    device=\"cpu\",\n",
        "    num_epochs=30,\n",
        "    patience=5,\n",
        "    save_path=\"best_detector.pth\"\n",
        "):\n",
        "    best_map = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images = images.to(device)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # в train режиме\n",
        "            loss = detection_loss(outputs, targets, anchors, device=device)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        val_map = validate(model, val_loader, device=device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Val mAP: {val_map:.4f}\")\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        if val_map > best_map:\n",
        "            best_map = val_map\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"  → New best mAP: {best_map:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    return best_map"
      ],
      "metadata": {
        "id": "cw32Yqn-QqmZ"
      },
      "id": "cw32Yqn-QqmZ",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Устройство\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Модель\n",
        "model = Detector(\n",
        "    backbone_model_name=\"efficientnet_b0\",\n",
        "    num_classes=4,\n",
        "    anchor_sizes=((32,), (64,), (128,)),\n",
        "    anchor_ratios=((0.5, 1.0, 2.0),) * 3,\n",
        "    input_size=(640, 640),\n",
        "    unfreeze_last_k=2\n",
        ").to(device)\n",
        "\n",
        "# Анкоры (предвычисленные)\n",
        "anchors = model.anchors  # должен быть в XYXY\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "# Даталоадеры (должны возвращать images: List[Tensor], targets: List[Dict])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Обучение\n",
        "best_map = train_detector(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    anchors=anchors,\n",
        "    device=device,\n",
        "    num_epochs=30,\n",
        "    patience=5\n",
        ")\n",
        "\n",
        "print(f\"Best mAP: {best_map:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "6uQsR4uiQxb7",
        "outputId": "40cec84b-a0b6-4c4b-d0d8-c87cea2e8486"
      },
      "id": "6uQsR4uiQxb7",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "Epoch 1:   0%|          | 0/116 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1127635896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Обучение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m best_map = train_detector(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-84770092.py\u001b[0m in \u001b[0;36mtrain_detector\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, anchors, device, num_epochs, patience, save_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea5c0ec-c738-4c11-b6ec-dd1bf9a9491d",
      "metadata": {
        "id": "6ea5c0ec-c738-4c11-b6ec-dd1bf9a9491d"
      },
      "source": [
        "Ниже определена вспомогательная функция для валидации качества. Можете использовать `Runner.validate`. Важное уточнение, ей нужен метод для фильтрации предсказаний. Можете тоже скопировать его из семинара, если он у вас не менялся."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299b193f-b697-449d-b4c0-3ef57c77eb7e",
      "metadata": {
        "id": "299b193f-b697-449d-b4c0-3ef57c77eb7e"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(dataloader, filter_predictions_func, box_format=\"xyxy\", device=\"cpu\", score_threshold=0.1, nms_threshold=0.5, **kwargs):\n",
        "    \"\"\" Метод для валидации модели.\n",
        "    Возвращает mAP (0.5 ... 0.95).\n",
        "    \"\"\"\n",
        "    self.model.eval()\n",
        "    # Считаем метрику mAP с помощью функции из torchmetrics\n",
        "    metric = MeanAveragePrecision(box_format=box_format, iou_type=\"bbox\")\n",
        "    for images, targets in tqdm(dataloader, desc=\"Running validation\", leave=False):\n",
        "        images = images.to(device)\n",
        "        outputs = self.model(images)\n",
        "        predicts = filter_predictions_func(outputs, score_threshold, nms_threshold, **kwargs)\n",
        "        metric.update(predicts, targets)\n",
        "    return metric.compute()[\"map\"].item()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}